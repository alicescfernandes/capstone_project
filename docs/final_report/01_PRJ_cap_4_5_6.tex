%%________________________________________________________________________
%% LEIM | PROJETO
%% 2022 / 2013 / 2012
%% Modelo para relatório
%% v04: alteração ADEETC para DEETC; outros ajustes
%% v03: correção de gralhas
%% v02: inclui anexo sobre utilização sistema controlo de versões
%% v01: original
%% PTS / MAR.2022 / MAI.2013 / 23.MAI.2012 (construído)
%%________________________________________________________________________


%%________________________________________________________________________
\chapter{Implementação do Modelo}
\label{ch:implementacaoDoModelo}
%%________________________________________________________________________

Neste capítulo iremos descrever de forma detalhada como foi feita a transição do modelo proposto para a implementação  da aplicação. A abordagem seguida foi iterativa, com várias fases de experimentação e validação, sempre em articulação com os objetivos definidos para o projeto e os dados reais obtidos anteriormente.

Começamos por abordar o processo de análise e classificação dos dados, uma vez que a estrutura e qualidade da informação recebida tiveram um impacto direto na forma como os dados seriam processados, armazenados e, mais tarde, visualizados. A seguir, exploramos a construção do pipeline de transformação, a lógica de organização dos ficheiros e o modo como garantimos que cada carregamento fosse tratado de forma automática. Por fim, explicamos as decisões que nos levaram à escolha de representações gráficas específicas, assim como os mecanismos que permitem ao utilizador interagir com os dados.

\section{Análise e Transformação dos dados}

O processo de análise começou após a reunião inicial com os orientadores do projeto, onde recebemos os ficheiros com que iamos trabalhar. Tratar este ficheiros implicou várias fases de análise, onde começamos a pensar em estratégias para conseguir extrair dados e possiveis gráficos, e tornar este processo modular e automatizado.

\subsection{Análise inicial dos dados}
\label{sec:analiseInicial}

Após termos recebido os dados, fizemos uma primeira análise com o objetivo de perceber a informação recebida e os próximos passos a tomar. No total, recebemos 30 ficheiros \gls{xlsx} que foram exportados da plataforma, correspondentes às diferentes secções da plataforma de simulação, com vários indicadores como segmentos de mercado, características de produto, vendas, entre outros. Cada ficheiro pode conter várias folhas de cálculo, que no total são 103 folhas de cálculo, organizadas por marcas, cidades entre outros marcadores.

Durante esta análise, percebemos que:

\begin{itemize}
    \item Os nomes dos ficheiros e das folhas variam, mas seguem uma estrutura relativamente consistente;
    \item Alguns ficheiros apresentam estruturas de dados semelhantes, mas com diferenças no nível de detalhe ou na organização das colunas e linhas;
    \item Existiam ficheiros com dados que iriam precisar de mais transformação uma vez que representavam vários tipos de unidades (como por exemplo percentagens, valores monetários, valores relativos, etc) na mesma folha de cálculo;
    \item Dados com muito detalhe, e com várias colunas sem representação (células marcadas com "X", linhas com valores nulos e fundo amarelo).
\end{itemize}

Com base nesta primeira análise, concluímos que seria necessário:
\begin{itemize}
    \item Fazer uma normalização da informação recebida para garantir uma utilização consistente da aplicação;
    \item Separar os vários ficheiros possiveis em grupos, de forma a identificar dados em comum que pudéssemos aplicar \textit{templates} de gráfico;
    \item Guardar os dados extraídos num formato mais prático, e que permita uma filtragem dinâmica da informação carregada (de forma a facilitar a visualização dos dados).
    \item Em algumas séries de dados, identificamos linhas e colunas que se podiam remover devido a serem redundantes, ou por representarem informação que já é representada na mesma folha (como por exemplo colunas de valores totais, linhas que repetiam as marcas, ou outras discutidas com os orientadores);
\end{itemize}

Esta primeira análise serviu essencialmente para estruturar o modo como iríamos tratar os diferentes ficheiros que os utilizadores podem submeter, ou seja, deu-nos uma base para sistematizar como iriamos transformar a informação de forma lógica.

Com isso em mente, optámos por agrupar a informação em várias categorias, com base na natureza e estrutura dos dados de cada folha de cálculo. A cada uma dessas categorias passámos então a associar um tipo de gráfico específico, o que nos permitiu criar uma espécie de conjunto de templates reutilizáveis. Esta abordagem facilita bastante o processo, porque conseguimos aplicar esses templates de forma programática, sem precisar de decisões manuais ficheiro a ficheiro.

\subsection{Classificação dos dados}

A análise inicial dos ficheiros fornecidos permitiu-nos perceber que, apesar da informação ser diferente,  existiam padrões na forma como estava estruturados. Com base nisso, tomámos a decisão de agrupar os ficheiros em \textit{buckets} ou categorias. Cada um desses grupos ficou associado a um \textit{template} gráfico específico, o que nos permite não só uniformizar a apresentação dos dados, como também automatizar o processo de transformação e visualização a partir dos ficheiros.

Os grupos definidos foram os seguintes: \textbf{simples}, \textbf{duplo}, \textbf{balanços}, \textbf{setores} e \textbf{análise específica}. Vamos então descrever o significado de cada grupo e identificar o que é comum entre os gráficos pertencentes a cada um deles.

O grupo \textbf{simples} inclui ficheiros em que a estrutura é mais direta, geralmente com apenas duas colunas: uma coluna que representa uma categoria (como por exemplo empresas ou segmentos de mercado) e uma coluna numérica. Nestes casos, optámos por gráficos de barras, uma vez que o objetivo principal é comparar rapidamente valores individuais entre categorias. Nos ficheiros que avaliamos, não encontramos séries temporais, pelo que não justificou gráficos de linhas. Também nesta categoria, alguns dados eram séries mais específicas (como dados relativos a médias e medianas) mas mantinham a mesma estrutura de dados, pelo que foram classificados como \textbf{simples} mas utilizaram gráficos diferentes como gráficos de diagramas e quartis e gráficos de setores (também conhecidos como \textit{pie charts}).

O grupo \textbf{duplo} refere-se a ficheiros onde existem múltiplas séries de dados associadas à mesma categoria. Ou seja, para cada categoria, existem vários valores (ou várias séries) que precisam de ser representados lado a lado. Para estes casos, a escolha que fizemos foi apresentar gráficos de barras agrupadas e barras empilhadas, permitindo uma comparação direta entre diferentes séries de dados para a mesma categoria, e permite visualizar todos os dados relacionados com essa categoria.

O grupo \textbf{balanços} abrange ficheiros associados a séries de dados financeiros, onde faz sentido representar aumentos e diminuições de valores ao longo de um processo ou período. Para estes ficheiros, o \textit{template} selecionado foi o gráfico de cascata (ou um gráfico \textit{financial waterfall}), dado que este tipo de visualização representa as várias componentes do valor total numa forma fácil de perceber.

O grupo \textbf{setores} inclui ficheiros onde a informação está agrupada em em duas e três colunas (como por exemplo dados por empresa, marca e cidade). Nestes casos, faz sentido usar gráficos de setores e barras agrupadas,  dependendo da necessidade de comparar proporções entre segmentos.

Finalmente, o grupo de \textbf{análise específica} são ficheiros que não se encaixam diretamente nos formatos anteriores, como análises mais detalhadas por cidade, por segmento ou quando os dados incluem várias unidades na mesma folha (percentagens com euros). Para estes casos, o processo passou por simplificar a informação de modo a encaixar num dos grupos acima ou excecionalmente aplicar um novo tipo de gráfico, e a análise foi feita manualmente para cada caso. Para casos em que a não era possível aplicar um \textit{template} de gráfico, não foi possivel simplificar a informação e não encontrámos uma representação gráfica que fosse fácil de interpretar, optámos por não aplicar nenhum gráfico, e apenas apenas mostrar uma tabela interativa com os dados onde o utilizador podia filtrar os dados por diferentes colunas.

No final, dos 30 ficheiros recebidos, a classificação foi a seguinte:

\begin{figure}[h]
    \centering
    \includegraphics[max width=\textwidth]{./img/stats1}
 \caption{Classificação dos ficheiros - contagem final dos grupos}
 \end{figure}

Em termos de representações utilizadas, a distribuição é a seguinte:
\begin{figure}[h]
    \centering
    \includegraphics[max width=\textwidth]{./img/stats2}
 \caption{Classificação dos ficheiros - contagem final de representações utilizadas}
 \end{figure}

\subsection{Transformação dos dados}

Com a classificação de dados feita, foi necessário desenvolver um \textit{pipeline} de processamento que permitisse normalizar e transformar os dados dos ficheiros em um formato mais adequado para visualização e para guardar na base de dados. Este processo foi desenvolvido com recurso a biblioteca Pandas (Python), que oferece métodos para manipulação e transformação de dados. Esta \textit{pipeline} funciona de forma assincrona, e é chamada pelo \textit{backend} quando um ficheiro é carregado, de forma a não bloquear a interface do utilizador.

A \textit{pipeline} de processamento foi dividida em duas fases, cada uma com um objetivo específico na transformação dos dados.

\subsection{Extração e Normalização Inicial}

Nesta fase, os dados são extraídos dos ficheiros \gls{xlsx} e normalizados para um formato consistente:

\begin{itemize}
    \item Extração do título do gráfico a partir da primeira linha de cada folha
    \item Identificação de cada folha por num nome único (derivado do nome da folha de cálculo), para que possamos identificar os dados de forma única na aplicação (ou seja, um \textit{slug} que mapeia exclusivamente a uma série de dados). Este passo é importante para garantir que cada série de dados é tratada de forma única e consistente, e para conseguirmos manter uma configuração para cada série de dados.
    \item Normalização dos cabeçalhos das colunas (remoção de quebras de linha, espaços extras)
    \item Remoção de colunas baseadas na análise feita na secção \ref{sec:analiseInicial}
    \item Normalização dos dados nas células (remoção de quebras de linha, espaços duplos)
    \item Tratamento de valores nulos e vazios
    \item Normalização de valores numéricos com precisão configurável
    \item Manutenção da ordem original das colunas para preservar a estrutura dos dados. A biblioteca Pandas automaticamente ordena as colunas, pelo que é necessário manter a ordem original das colunas para preservar a estrutura dos dados.
    \item Transformação de dados marcados como "X" em formato binário (0/1) para indicar presença/ausência de valores
    \item Normalização de valores decimais num valor escolhido pelo utilizador (com o valor por omissão a 9)
\end{itemize}

\subsection{Processamento Específico por Tipo}

Após a normalização inicial, os dados passam por transformações específicas baseadas no seu tipo, e no tipo de gráfico que pretendemos representar:

\begin{itemize}
    \item Aplicação de transformações específicas baseadas no tipo de gráfico identificado (financeiro, percentagens, valores relativos entre outros)
    \item Processamento para balanços financeiros, incluindo cálculos de percentagens e valores relativos de outros valores (aplicar somas e subtrações como descrito na folha de cálculo)
    \item Transformação dos dados para conseguir representar categorias e a relações entre elas (como por exemplo a relação entre marcas e segmentos de mercado e cidades)
    \item Para folhas que continham multiplos \textit{quarters}, optámos por escolher o \textit{quarter} para o qual a folha foi carregada, sendo que essa configuração pode ser trocada em código.
\end{itemize}

De forma a tornar modular toda a lógica de normalização de dados, decidimos criar configurações na aplicação que mapeiam ficheiros a gráficos e as suas transformações e outras configurações associadas a colunas que são usadas para representar os dados, sendo que cada série de dados é identificada unicamente pelo seu \textit{slug}. Esta configuração não é guardada na base de dados, e só pode ser editada em código, porque os dados extraidos, para serem mostrados, precisam de ser processados de forma especifica de modo a que consigam ser representados, e a sua visualização foi pensada para ser um gráfico especifico, pelo que não era possivel suportar várias representações para a mesma série de dados. Após os dados serem processados, já não é possivel alterar a visualização dos dados, porque o processamento adapta a informação que recebe para a visualização pretendida.

\subsection{Conversão e Armazenamento em Base de Dados}
\label{sec:armazenamentoDados}

Após o processamento dos dados ser feito, os mesmos são guardados como \gls{json} num campo especifico \textit{JSONField} da base de dados que decidimos utilizar, PostgreSQL. Estes dados guardados tentam ser os mais próximos ao dados mostrados ao utilizador, mas sem perder informação quando contem várias séries.

Para garantir a consistência dos dados e evitar conflitos de ficheiros com o mesmo nome a serem carregados para o mesmo \textit{quarter}, foi implementado um mecanismo que permite marcar ficheiros como processados / não processados e como correntes / não correntes. Quando um utilizador carrega um ficheiro, este é marcado como processado e como corrente, e os dados são guardados na base de dados. Quando um utilizador carrega um novo ficheiro com o mesmo nome, o novo é marcado como corrente, e o antigo é marcado como não corrente. Em nenhum momento é apagado versões anteriores dos dados nem os ficheiros originais que foram carregados. A aplicação tem depois um mecanismo em que só considera ficheiros marcados como correntes para serem mostrados ao utilizador.

Isto permite recuperar versões anteriores dos dados quando o utilizador apaga um ficheiro ou quando existe algum erro no processamento, uma vez que os dados resultantes só são marcados como correntes quando são extraidos e processados com sucessos.

Este sistema de processamento permitiu transformar os dados ficheiros \gls{xlsx} em um formato estruturado e normalizado, facilitando a visualização e análise dos dados através da aplicação web. A modularidade da \textit{pipeline} também permitiu adicionar passos de normalização conforme necessário, mantendo a consistência dos dados processados.

No final, obtemos dados que são práticos de representar, próximos ao formato final que pretendemos mostrar ao utilizador, e que podem ser facilmente utilizados pela aplicação web para mostrar os gráficos.

\begin{figure}[H]
\centering
\includegraphics[max width=\textwidth]{./img/before}
\caption{Excerto de uma folha de cálculo recebida}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[max width=\textwidth]{./img/after}
\caption{Gráfico extraido da mesma folha acima com filtro aplicado para o Quarter 5}
\end{figure}

\section{Desenvolvimento da Aplicação Web}

Esta secção foca-se na estrutura e funcionamento da aplicação web, tanto o \textit{backend} em Django como a interface construída com \gls{html}, \textit{WebComponents} e Flowbite.

\subsection{Arquitetura da aplicação (Backend)}

A aplicação web foi desenvolvida utilizando o framework Django como já descrito na secção \ref{sec:fundamentos}. A arquitetura foi desenhada para garantir o isolamento dos dados por utilizador e uma gestão dos ficheiros e dados processados.

\subsubsection{Modelos de Dados}

A aplicação utiliza três modelos principais para gerir os dados, cada um com um papel específico na gestão e organização da informação. Estes modelos formam a base da aplicação, permitindo uma estrutura clara e organizada dos dados. Como o Django utiliza o \gls{orm} para gerir as relações entre os modelos, cada classe representa também um modelo na base de dados, que é apresentado na figura \ref{fig:er-diagram}. A classe \textit{User} é uma classe que já vem incluida com o Django, e é utilizada para gerir os utilizadores e sessões da aplicação.

\begin{figure}[H]
    \centering
    \includegraphics[max width=\textwidth]{./img/er-diagram.png}
 \caption{Diagrama de entidade-relação da aplicação}
 \label{fig:er-diagram}
 \end{figure}


O nosso modelo de dados é composto por três modelos principais: \textit{Quarter}, \textit{ExcelFile} e \textit{ChartData}, cada um com um papel específico no contexto da aplicação.

\begin{itemize}
    \item \textbf{Quarter:} Representa um trimestre específico para um utilizador. Cada \textit{quarter} tem:
    \begin{itemize}
        \item Um número único por utilizador
        \item Uma precisão configurável para valores numéricos (por omissão 9 casas decimais)
        \item Um \gls{uuid} único para identificação
        \item Relação com o utilizador que o criou
    \end{itemize}

    \item \textbf{ExcelFile:} Representa um ficheiro Excel carregado para um \textit{quarter} específico:
    \begin{itemize}
        \item Armazena o ficheiro físico em pastas organizadas por \gls{uuid}
        \item Mantém o estado de processamento (processado/não processado)
        \item Guarda metadados como a secção do ficheiro
        \item Relaciona-se com o \textit{quarter} e o utilizador
        \item Controla qual versão dos dados está ativa (corrente ou não)
    \end{itemize}

    \item \textbf{ChartData:} Armazena os dados processados de cada folha do Excel:
    \begin{itemize}
        \item Guarda os dados em formato \gls{json}
        \item Mantém a ordem original das colunas
        \item Guarda metadados como o nome da folha e o \textit{slug} para identificar os dados unicamente
        \item Relaciona-se com o ficheiro Excel de origem e o utilizador
    \end{itemize}
\end{itemize}

A utilização de identificadores \gls{uuid} foi importante uma vez que, para além de identificar unicamente cada instancia da entidade, faz com que não seja possivel aceder aos dados só a aumentar o identificador, como se fosse o caso se o identificador fosse um número inteiro sequencial. O Django já disponibiliza uma chave primária em cada modelo por omissão, que é um número sequencial inteiro.

\subsubsection{Gestão de Utilizadores}

A aplicação utiliza o mecanismo de utilizadores que já vem incluido com o Django. Este sistema oferece uma camada de segurança, garantindo que apenas utilizadores autenticados possam aceder aos dados. A autenticação na aplicação é feita através de nome de utilizador e palavra-passe e as sessões são geridas pelo Django.

Para garantir a segurança dos dados, todas as rotas que requerem autenticação estão decoradas com \texttt{@login\_required}. Além disso, a aplicação isola dados dados por utilizador, garantindo que cada utilizador só pode aceder aos seus próprios. Este isolamento é implementado através de filtros nas pesquisas, que são aplicados automaticamente em todas as operações de leitura e escrita que são feitas.

Os utilizadores podem criar conta na aplicação, sendo que podem criar conta para si ou para um grupo de utilizadores. A aplicação não faz distinção entre uma conta para um utilizador ou para uma conta para multiplos utilizadores.

\subsubsection{Gestão de Ficheiros e Processamento de dados}

Para conseguirmos manter vários ficheiros em disco, tivemos de pensar num mecanismo capaz de manter ficheiros organizados. Para isso, desenvolvemos um mecanismo que garante que os dados carregados pelos utilizadores ficam organizados em pastas isoladas. Em vez de depender apenas do nome do ficheiro (o que podia facilmente gerar conflitos ou sobreposições de nomes), a aplicação cria automaticamente uma pasta associada a um \gls{uuid}, o que garante que cada ficheiro fica isolado do resto. Isto evita situações em que dois ficheiros com nomes iguais se sobrepõem e, ao mesmo tempo, facilita bastante a estruturação interna por \gls{quarter}. Assim que o ficheiro é carregado, o processamento é iniciado, sem depender de passos manuais , ou seja, o utilizador só precisa de fazer o \textit{upload} e o resto é tratado pela aplicação.

Para além disso, a aplicação guarda registos de todos os ficheiros carregados. Quando se carrega um novo ficheiro para substituir outro com o mesmo nome ou que resulta nos mesmos dados, a aplicação não apaga o anterior, como já foi descrito no capítulo (\cf, capítulo \ref{sec:armazenamentoDados}). Esta versão antiga pode ser usada mais tarde, caso seja necessário recuperar informação, validar alterações, ou aplicar novamente a normalização dos dados. Esta funcionalidade não foi uma funcionalidade pensada para ser exposta diretamente ao utilizador, e não faz parte do uso normal, mas surgiu como consequência da arquitetura escolhida (ficheiros correntes e não correntes).

\subsubsection{Endpoints para comunicação com o frontend}

A aplicação expõe um conjunto de \textit{endpoints} \textit{rest} que permitem a interação com o frontend. Estes \textit{endpoints} são organizados de forma lógica, com rotas específicas para diferentes funcionalidades:

\begin{itemize}
    \item \texttt{quarters/new/}: Endpoint para criação de um novo \textit{quarter}.
    \item \texttt{quarters/edit/<uuid:uuid>/}: Permite editar os detalhes de um \textit{quarter} já existente, identificado pelo seu \gls{uuid}.
    \item \texttt{quarters/delete/<uuid:uuid>/}: Endpoint para apagar um \textit{quarter} específico.
    \item \texttt{quarters/}: Endpoint para listar todos os \textit{quarters} do utilizador.
    \item \texttt{quarters/files/delete/<uuid:uuid>/}: Endpoint para remover um ficheiro associado a um \textit{quarter}, usando o \gls{uuid} do ficheiro.
  \end{itemize}
  
A API inclui \textit{endpoints} para gestão de \textit{quarters} (\texttt{/api/\textit{quarters}/}) e visualização de gráficos (\texttt{/api/chart/})  O \textit{endpoint} para visualização de gráficos (\texttt{api/chart/}) é o mais importante, e é o que é utilizado para mostrar os gráficos na aplicação web, e permite uma série de parâmetros para filtrar os dados e mostrar gráficos de diferentes tipos. Estes endpoints são usados com recurso a pedidos HTTP (utilizando \texttt{fetch}) e também como páginas que aceitam parâmetros por \gls{url} (como por exemplo em formulários através do atributo \textttt{action}).

\begin{lstlisting}[language=\gls{html}, caption={Excerto do código \gls{html} do formulário de edição de \textit{quarter}}]
    <form method="post" action="/quarters/edit/b12fdf1f-8ce3-4050-a28f-07e444e15042/" id="edit-quarter-form" class="upload-form-wrapper" enctype="multipart/form-data">
      {% csrf_token %}
      <div class="flex justify-end">
        <button type="submit" class="upload-modal-submit-btn">Save</button>
      </div>
    </form>
    \end{lstlisting}

Quando a aplicação carrega, não mostra logo todos os gráficos, mas sim um \textit{skeleton} que depois é substituido pelo gráfico, permitindo uma melhor experiência do utilizador. Esse carregamento é feito de forma assincrona, por Javascript, e utiliza o \textit{endpoint} para visualização de gráficos para obter a configuração final de um gráfico especifico a ser mostrado. A configuração retornada é especifica para a biblioteca \textit{Plotly}, que depois, juntamente com o restante código Javascript, consegue mostrar o gráfico e aplicar filtros com base nos parâmetros passados.

A comunicação com o \textit{backend} é feita através de chamadas assíncronas aos \textit{endpoints} desenvolvidos, passando parâmetros como o \textit{quarter} selecionado e os filtros ativos. O \textit{backend} devolve a estrutura necessária para renderização com \textit{Plotly}, assegurando que cada gráfico é gerado com os dados e configurações corretas.

\subsection{Interface Gráfica (Frontend)}

A interface da aplicação foi desenhada com foco na usabilidade e consistência visual, através da utilização de tecnologias modernas como \textit{WebComponents} e através de um \textit{design system}, Flowbite. O objetivo era garantir uma experiência responsiva, adequada para desktop e que suportasse minimamente dispositivos móveis, sem comprometer a performance nem a clareza na apresentação dos dados.

\subsubsection{Layout e Linguagem visual}

Um design system é uma biblioteca que oferece um conjunto de componentes reutilizáveis que podem ser usados em interfaces visuais. A escolha do Flowbite, que utiliza internamente a biblioteca Tailwind CSS, acelerou o desenvolvimento e simplificou a criação da interface visual, e de elementos como modais, formulários e botões. A coerência visual é reforçada por tipografia, cores e espaçamentos que já vem definidos por omissão na biblioteca.

A navegação é feita através de uma barra lateral, que mostra os gráficos disponíveis, enquanto modais são usados para operações críticas como confirmações e carregamento de ficheiros. Outras páginas são disponibilizadas numa barra de navegação no topo da página, que permite os utilizadores aceder à pagina de carregamento de ficheiros. Para o desenvolvimento do \gls{css} foi utilizada a linguagem SCSS, que permite desenvolver estilos de forma mais rápida e mais organizada, que depois é transpilada (através de um \textit{build system} como o ESBuild\cite{esbuild}) para \gls{css} normal.

Quando não existe gráficos a mostrar, é mostrada ao utilizador uma mensagem que indica que não existe ficheiros carregados, com uma hiperligação a redirecionar para a página de carregamento de ficheiros.

\subsubsection{WebComponents}

A camada de visualização de dados é composta por componentes Web, cada um encapsulando toda a sua a lógica, incluindo integrações com as bibliotecas \textit{Plotly} e \textit{Datatables} . Esta abordagem garante isolamento entre componentes, facilita a reutilização e simplifica a manutenção do código Javascript. 

A especificação \textit{WebComponents}\cite{webcomponents} permite criar os nossos nós \gls{dom}, e estende as interfaces \gls{dom} já existentes. Esta funcionalidade permite criar componentes que se comportam como elementos \gls{html}, e que podem ser utilizados como tal, e que também podem receber estilos e outras propriedades e atributos e é suportada já pelos os \textit{browsers} mais usados (como pode ser consultado no site \textit{Can I Use}\cite{caniuse} que é um site que indica o suporte de uma funcionalidade em diferentes \textit{browsers}).

\begin{figure}[H]
    \centering
    \includegraphics[max width=\textwidth]{./img/caniuse}
 \caption{Tabela de suporte - \textit{WebComponents}}
 \end{figure}

Na nossa aplicação, os \textit{WebComponents} são usados como elementos \gls{html} normais, que recebem apenas o gráfico que é para mostrar. Ao carregar, fazem uma chamada a um \textit{endpoint} do \textit{backend}, que retorna a configuração do gráfico, que é utilizada para mostrar o gráfico.

\begin{figure}[H]
    \centering
    \includegraphics[max width=\textwidth]{./img/webc}
 \caption{Utilização de \textit{WebComponents} na aplicação}
 \end{figure}

A comunicação com o \textit{backend} é feita através de chamadas assíncronas aos \textit{endpoints} desenvolvidos, passando parâmetros como o \textit{quarter} selecionado e os filtros ativos. O \textit{backend} devolve a configuração do gráfico para renderização com \textit{Plotly}, assegurando que cada gráfico é gerado com os dados e configurações corretas, como pode ser observado no diagrama de sequência \ref{fig:sequence}.

\begin{figure}[H]
    \centering
    \includegraphics[max width=\textwidth]{./img/sequence}
 \caption{Diagrama de sequência da comunicação com o \textit{backend}}
 \label{fig:sequence}
\end{figure}

A biblioteca \textit{Plotly} suporta uma configuração declarativa, ou seja, apenas precisamos de retornar um objeto estático, e através dessa configuração, a biblioteca consegue criar a visualização

\begin{lstlisting}[language=Javascript, caption={Excerto de uma configuração para um gráfico com a utilização da biblioteca \textit{Plotly}}]
  var trace1 = {
    x: ['giraffes', 'orangutans', 'monkeys'],
    y: [20, 14, 23],
    name: 'SF Zoo',
    type: 'bar'
  };
  
  var trace2 = {
    x: ['giraffes', 'orangutans', 'monkeys'],
    y: [12, 18, 29],
    name: 'LA Zoo',  
    type: 'bar'
  };
  
  var data = [trace1, trace2];
  var layout = {barmode: 'stack'};
  Plotly.newPlot('myDiv', data, layout);
\end{lstlisting}

Os gráficos são carregados de forma progressiva (\textit{lazy load}). Apenas são mostrados quando o nó entra dentro do \textit{viewport} do utilizador. O \textit{lazy load} permite que os servidor não fique sobre-carregado com pedidos, e que no lado do cliente, a memória utilizada seja minimizada visto que não são carregados todos os gráficos de uma vez.	

As bibliotecas \textit{Plotly} e \textit{Datatables} foram integradas diretamente com os \textit{WebComponents}\cite{webcomponents}, fazendo com que o WebComponent consiga mostrar gráficos e tabelas de forma encapsulada. Esta integração permite que, quando o carregamento de um gráfico é iniciado, que seja apresentado  um \textit{skeleton} (uma representação visual com um \textit{spinner} e com a largura e altura aproximada de um gráfico) que é trocada pelo o gráfico em si quando os dados ficam disponíveis. A ideia deste \textit{skeleton} é que o utilizador veja que o gráfico está a ser carregado, e que não cause um grande deslocamento do gráfico na interface (\textit{layout shifting}) quando os dados estão prontos.

\subsubsection{Responsividade em dispositivos móveis}

Toda a interface foi pensada para ser utilizada em computadores devido à quantidade de gráficos que são apresentados. A interface adapta-se bem a dispositivos moveis, mas devido ao espaço disponivel, não é possível garantir uma boa experiencia de utilização em dispositivos moveis. A aplicação tem, essencialmente, duas \textit{media queries}. Uma \textit{media query} que até aos 1023px que inclui telemóveis, tablets e ecrãs pequenos, e uma \textit{media query} a partir dos 1024px que inclui computadores e ecrãs maiores.

\begin{figure}[H]
    \centering
    \includegraphics[max max width=\textwidth]{./img/res_1023}
 \caption{Media query até 1023px}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[max max width=\textwidth]{./img/res_1920}
 \caption{Media query a partir dos 1024px}
\end{figure}

Estes \textit{media queries} são as mais comuns, garantindo uma boa experiência de utilização em todos os ecrãs, sendo que não tivemos de fazer código explicitamente para a aplicação se adaptar para dispositivos móveis. Os filtros permitem alterar os gráficos em tempo real, com \textit{feedback} visual imediato não sendo necessário recarregar a página. No momento em que o filtro troca, é feito um pedido a API de gráficos que retorna um novo conjunto de dados para mostrar, com a configuração do gráfico atualizada.

\subsubsection{Carregamento de ficheiros}

A organização dos dados foi pensada para refletir a lógica do projeto Marketplace Simulations. Cada utilizador pode criar múltiplos \textit{quarters} no momento em que carrega um ficheiro. Estes \textit{quarters} funcionam como buckets isolados onde são armazenados os ficheiros carregados. A navegação entre \textit{quarters} é facilitada por setas laterais em cada gráfico apresentado. A criação e remoção de \textit{quarters}, bem como o carregamento de ficheiros, é acompanhada por feedback visual no momento da ação.

Para verificar se os ficheiros são do tipo correto, decidimos validar através do \textit{MIME type}, que é um identificador  utilizado para descrever o tipo de conteúdo de um ficheiro. Os \textit{MIME types} seguem o formato \texttt{tipo/subtipo}, como por exemplo \texttt{text/csv} para ficheiros CSV ou \texttt{application/\allowbreak vnd\allowbreak.open\allowbreak xmlV formats-\allowbreak officedocument.\allowbreak spreadsheetml.\allowbreak sheet} para ficheiros Excel no formato \texttt{.xlsx}. Esta abordagem permite garantir que os ficheiros carregados correspondem realmente ao formato esperado, independentemente da extensão do nome do ficheiro que pode ser facilmente manipulada. 

Ao validar o \textit{MIME type} do ficheiro, conseguimos rejeitar ficheiros com o tipo errado. No contexto deste projeto, esta verificação era essencial, uma vez que apenas ficheiros \texttt{.xlsx} válidos devem ser processados. Assim, a validação por \textit{MIME type} contribui tanto para a segurança como para a integridade da aplicação.

Durante o carregamento, a interface valida se os ficheiros são do tipo correto. Apenas ficheiros dos mime type \texttt{application/vnd\allowbreak.openxmlformats\allowbreak officedocument\allowbreak.spreadsheetml\allowbreak.sheet} e \texttt{application/vnd.ms\allowbreak excel} são permitidos.

%%________________________________________________________________________
\chapter{Validação e Testes}
\label{ch:validacaoTestes}
%%________________________________________________________________________

A validação e testes da aplicação foram realizados através de múltiplas abordagens, garantindo uma cobertura de testes  da funcionalidade como da usabilidade da aplicação. Este capítulo descreve as diferentes estratégias de teste implementadas e os resultados obtidos.

\section{Testes de Acessibilidade}

A acessibilidade foi uma preocupação no desenvolvimento da aplicação, e para isso foram seguidas as diretrizes \gls{wcag} 2.1, que definem critérios técnicos para tornar os conteúdos web mais acessíveis a todos os utilizadores, incluindo pessoas com deficiências visuais, motoras, auditivas, cognitivas ou neurológicas. Para garantir a conformidade com estas diretrizes, recorremos a uma abordagem prática combinada de testes automáticos e validação manual. Entre as ferramentas utilizadas destacam-se duas principais: o Lighthouse e o Axe.

O Lighthouse, desenvolvido pela Google, é uma ferramenta open source que corre diretamente no Chrome DevTools ou como \gls{cli}. Permite auditar uma página web em várias categorias, nomeadamente Performance, SEO, Best Practices e, claro, Accessibility. A secção de acessibilidade do Lighthouse verifica, por exemplo, se os elementos têm contrastes suficientes, se os formulários estão corretamente identificadas com atributos aria-label, se há títulos estruturados (h1, h2, etc.) numa hierarquia lógica e se os elementos interativos (como botões e links) são acessíveis por teclado. O Lighthouse fornece uma pontuação de 0 a 100 e destaca problemas comuns com sugestões de como resolver.

Esta ferramenta Lighthouse, apesar de não ser tão detalhada como a Axe, sinalizou alguns problemas, como a falta de compressão \gls{gzip} e alguns erros de acessibilidade e \gls{seo}. Estes problemas foram resolvidos (fazendo alterações na configuração do servidor Nginx e no código da aplicação), e a ferramenta Lighthouse passou a dar uma pontuação melhor nos vários critérios que avalia.

\begin{figure}[H]
\centering
\includegraphics[max width=\textwidth]{./img/lh_before}
\caption{Testes de acessibilidade com a ferramenta Lighthouse}
\end{figure}

TODO: Adicionar imagem do Lighthouse após as correções.
\begin{figure}[H]
\centering
\includegraphics[max width=\textwidth]{./img/lh_before}
\caption{Testes de acessibilidade com a ferramenta Lighthouse após as correções}
\end{figure}

Já o Axe, da empresa Deque, é uma biblioteca de testes de acessibilidade mais focada, muito utilizada em ambientes de desenvolvimento profissional. Pode ser usada como extensão do browser ou integrada com ferramentas de testes automatizados como Cypress ou Selenium. O Axe segue os critérios \gls{wcag} com mais profundidade e reporta violações como a ausência de nomes acessíveis (\textit{accessible names}), uso incorreto de landmarks \gls{aria}, falhas de foco, ou elementos visuais sem correspondência textual. Além disso, o Axe permite navegar entre os erros diretamente na interface do browser, o que torna o processo de debugging muito mais fácil.

Nos primeiros testes feitos com a ferramenta Axe, as ferramentas sinalizaram alguns problemas relativos a acessibilidade, como a falta de textos alternativos para alguns elementos visuais, e os niveis dos titulos não estavam a ser usados de forma adequada.

\begin{figure}[H]
    \centering
    \includegraphics[max width=\textwidth]{./img/axe}
    \caption{Testes de acessibilidade com a ferramenta Axe}
    \end{figure}


Após resolver os problemas indicados pela ferramenta Axe, conseguimos então melhorar a acessibilidade da aplicação, como se pode ver na imagem abaixo.

\begin{figure}[H]
\centering
\includegraphics[max width=\textwidth]{./img/axe_after}
\caption{Testes de acessibilidade com a ferramenta Axe após as correções}
\end{figure}

No nosso caso, ambas as ferramentas foram utilizadas de forma complementar: o Lighthouse ajudou-nos a identificar problemas mais genéricos e dar uma visão rápida do estado da acessibilidade da aplicação, enquanto o Axe foi essencial para validar critérios mais técnicos e detetar problemas complexos que o Lighthouse pode não cobrir. Os testes foram feitos tanto em páginas com dados carregados como em estados vazios, e procurámos garantir que todas as interações essenciais da plataforma,  como o Carregamento de ficheiros, usabilidade e navegação entre gráficos fossem utilizáveis por utilizadores com leitores de ecrã e navegação apenas por teclado.

Desta forma, procurámos alinhar a aplicação com boas práticas de desenvolvimento inclusivo, respeitando não só normas técnicas mas também a responsabilidade social de criar interfaces acessíveis a todos, com foco nas seguintes funcionalidades:
\begin{itemize}
    \item Adição de textos alternativos para todos os elementos visuais
    \item Garantia de contraste adequado entre texto e fundo
    \item Implementação de navegação por teclado
    \item Estruturação semântica de \gls{html}
    \item Suporte a leitores de ecrã
\end{itemize}

\section{Testes Manuais}

Para complementar os testes das ferramentas acima, foi desenvolvido um conjunto abrangente de testes manuais utilizando a linguagem Gherkin para descrever os cenários de teste. Gherkin é uma linguagem estruturada, mas legível por humanos, que permite descrever comportamentos esperados do sistema em forma de cenários do tipo Dado-Quando-Então. Esta abordagem facilita a comunicação entre equipas técnicas e não técnicas, garantindo que todos os intervenientes compreendam os objetivos de cada teste.

Foi pedido a cinco testadores independentes para executarem estes testes manualmente, com base nos cenários escritos em Gherkin, e no final preencherem um formulário com os resultados de cada execução, incluindo observações e eventuais desvios face ao comportamento esperado. Os cenários de teste foram organizados em duas categorias principais:

\begin{itemize}
    \item Fluxos de utilizador (criação de conta, login, navegação)
    \item Operações com dados (carregar e apagar ficheiros, visualização de gráficos)
\end{itemize}

Cada cenário foi documentado usando a sintaxe Gherkin, como exemplificado abaixo. Os restantes cenários Gherkin foram incluídos no apêndice \ref{ch:cenariosGherkin}.

\begin{lstlisting}[language=Gherkin, caption={Excerto do código Gherkin do cenário de teste para a criação de uma conta}]
Scenario: Creating an account
	Given I access the page 
	And I don't have an account or logged in
	Then I should see the "Create Account" link
	When I click the "Create Account" link
	Then I should be redirected to the "Create Account" form
	And I should see the username field
	And I should see the password filed
	And I should see the confirm password field
	When I fill that form
	And I click "Save"
	Then I should be redirected to the "Login" page
\end{lstlisting}

Apesar desta amostra não ser muito representativa, foi possivel encontrar bugs na aplicação, como por exemplo, a falta de alguns dados em alguns gráficos, e alguns gráficos náo serem coerentes com a informação que estavam a mostrar. Estes bugs foram corrigidos assim que foram reportados pelos testadores.

TODO: Falta falar sobre a análise dos resultados dos testes manuais.

\section{Compatibilidade com Navegadores}

A compatibilidade com diferentes \textit{browsers} foi testada utilizando a plataforma BrowserStack, que permitiu testar a aplicação em múltiplos ambientes. Os testes foram realizados nas versões mais recentes dos principais \textit{browsers}:

\begin{itemize}
    \item Google Chrome (versões acima da 90)
    \item Mozilla Firefox (versões acima da 88)
    \item Microsoft Edge (versões acima da 90)
    \item Safari (versões acima da 18)
\end{itemize}

A aplicação é compatível com os \textit{browsers} testados, nas versões em que validámos o seu funcionamento.

%%_______________________________________________________________________
\chapter{Conclusões e Trabalho Futuro}
\label{ch:conclusoesTrabalhoFuturo}
%%________________________________________________________________________

O desenvolvimento deste projeto culminou numa aplicação web funcional que responde de forma direta e eficaz às necessidades dos estudantes do ISCAL no que diz respeito à análise dos dados exportados da plataforma Marketplace Simulations. Desde o início, o objetivo foi construir uma solução prática, que facilitasse a leitura e exploração da informação, e ao mesmo tempo fosse suficientemente robusta e escalável para crescer com os utilizadores e os dados.

\section{Conclusões}

Um dos pontos fortes deste projeto foi a implementação de um sistema automático para gerir ficheiros e transformar dados em gráficos. Esta funcionalidade, de os utilizadores poderem carregar ficheiros diretamente exportados do simulador e, a partir daí, obterem gráficos interativos, sem qualquer necessidade de formatação manual representa uma melhoria relação ao processo anterior, que era manual, demorado e propenso a erros. 

A decisão de usar Django como base para o \textit{backend} revelou-se acertada. Além de facilitar a autenticação e gestão de utilizadores, a  biblioteca permitiu implementar uma boa arquitetura, com separação entre as camadas frontend e \textit{backend}. A autenticação integrada, associada ao isolamento de dados por utilizador, garantiu uma camada extra de segurança sem complicar a lógica da aplicação. A API REST que foi construída permitiu uma comunicação fluida entre as várias partes do sistema, suportando atualizações dinâmicas dos gráficos sem impacto na performance.

No lado do frontend, optou-se por uma abordagem prática, usando bibliotecas para garantir a responsabilidade e usabilidade. A biblioteca \textit{Plotly} foi fundamental para criar gráficos interativos que, além de apelativos, ajudam os estudantes a interpretar os dados.

Durante o desenvolvimento surgiram alguns desafios técnicos. Um dos principais foi lidar com a diversidade estrutural dos ficheiros. Alguns traziam folhas com formatação irregular, nomes de colunas inconsistentes ou dados dificeis de perceber. Isso obrigou à criação de uma pipeline de normalização robusto e reutilizável que é uma componente essencial para garantir que os dados fossem processados corretamente e de forma previsível. Apesar da solução atual funcionar bem no contexto do projeto, poderá ser necessário alterar no futuro caso os ficheiros evoluam ou incluam novos tipos de dados. Outro ponto que pode ser melhorado é a interface de gestão de \textit{quarters}, que embora funcional, ainda exige alguma familiaridade com o conceito por parte do utilizador.

Para além do lado técnico, este projeto foi também uma excelente oportunidade de crescimento pessoal. Antes de começar, não tinha experiência prática com Django nem com bibliotecas como Pandas, e o contacto com estas tecnologias acabou por ser fundamental. Esta aprendizagem provou o valor da adaptação contínua e da experimentação prática como pilares fundamentais do desenvolvimento de software.

Em síntese, este projeto mostrou como a combinação de ferramentas modernas pode resultar numa solução escalável e com impacto direto na experiência de aprendizagem. A aplicação desenvolvida resolve um problema enfrentado pelos estudantes do ISCAL, e deixa aberta a porta para futuras melhorias e novos casos de uso. É uma base sólida, que pode ser facilmente estendida ou adaptada a outros contextos educacionais onde a análise de dados seja um desafio.

\section{Trabalho Futuro}

Apesar de o sistema atual já cumprir os objetivos definidos inicialmente, foram identificadas diversas oportunidades de evolução e expansão que poderão ser exploradas em futuras iterações do projeto. Estas propostas estão organizadas em diferentes áreas de desenvolvimento.

\subsection{Interface e Experiência do Utilizador}

Uma das direções mais imediatas para evolução está ao nível da interface. Pretende-se alargar o leque de visualizações disponíveis, incluindo novos tipos de gráficos, como dispersões, histogramas ou até mapas interativos, sendo que é sempre importante ter em mente a experiencia de utilização. Para além disso, seria interessante permitir a criação de \textit{dashboards} personalizáveis, onde cada utilizador poderia compor visualizações de acordo com os seus objetivos, e até juntar vários tipos de dados na mesma visualização.

Outro aspeto relevante prende-se com a configuração dinâmica dos gráficos. A ideia é deixar de apresentar visualizações estáticas e passar a oferecer ao utilizador a possibilidade de escolher, em tempo real, os tipos de gráfico, métricas e dimensões que pretende analisar. Esta abordagem aumentaria substancialmente a flexibilidade da ferramenta.

\subsection{Análise de Dados e Otimização de Performance}

Do ponto de vista analítico, uma evolução natural do sistema passa pela incorporação de técnicas de análise preditiva. A utilização de modelos de regressão ou algoritmos de aprendizagem automática simples poderá ajudar os estudantes a identificar padrões e antecipar tendências de mercado com base nos dados da simulação.

Em paralelo, importa melhorar a eficiência do sistema no processamento de dados de maior volume. Será relevante avaliar o uso de bibliotecas como \texttt{Dask} ou \texttt{Polars}, que oferecem soluções otimizadas para manipulação de grandes \textit{datasets}, mantendo tempos de resposta aceitáveis e garantindo uma boa experiência de utilização.

\subsection{Gestão de Utilizadores e Colaboração}

Ao nível da gestão da plataforma, propõe-se o reforço das funcionalidades de administração de utilizadores. Isto poderá incluir a criação de perfis diferenciados (por exemplo, administrador, editor, visualizador) e mecanismos de controlo de permissões.

Adicionalmente, seria interessante introduzir funcionalidades de colaboração entre utilizadores, nomeadamente a partilha de visualizações e filtros de dados. Este tipo de funcionalidade é particularmente útil em contextos académicos, onde os trabalhos são frequentemente realizados em grupo ao longo do semestre.

\subsection{Integração com Outras Plataformas}

Por fim, um caminho a explorar passa pela integração direta com a plataforma \textit{Marketplace Simulations}, de onde os dados são originalmente extraídos. Uma abordagem possível seria o desenvolvimento de extensões de \textit{browser} (para \textit{Chrome}, \textit{Firefox} ou \textit{Edge}) que permitam importar automaticamente os dados para o nosso sistema, eliminando etapas manuais do processo de upload.

De forma geral, estas propostas de melhoria mostram que o projeto pode continuar a evoluir significativamente, servindo como base para novas investigações ou aplicações.